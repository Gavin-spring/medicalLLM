import os
import re
from pathlib import Path
from typing import List, Dict, Optional
import fitz  # PyMuPDF
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

class MedicalIngestor:
    def __init__(self, base_dir: Path):
        # è‡ªåŠ¨å®šä½è·¯å¾„ï¼Œä¸ç®¡ä½ åœ¨å“ªè¿è¡Œè„šæœ¬éƒ½èƒ½æ‰¾åˆ° data æ–‡ä»¶å¤¹
        self.base_dir = base_dir
        if not self.base_dir.exists():
            raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {self.base_dir}")
            
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=100,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
        )

    def _extract_metadata(self, file_path: Path) -> Dict:
        # è·å–ç›¸å¯¹äº data/raw çš„ç›¸å¯¹è·¯å¾„ï¼Œä»è€Œå‡†ç¡®æå–ç±»åˆ«
        relative_path = file_path.relative_to(self.base_dir)
        category = relative_path.parts[0] # è·å–ç¬¬ä¸€çº§å­ç›®å½•å
        
        return {
            "source": file_path.name,
            "category": category,
            "disease": self._guess_disease(file_path.name),
            "path": str(relative_path)
        }

    def _guess_disease(self, filename: str) -> str:
        disease_map = {"BK": "BKV", "Alport": "Alport", "ç‹¼ç–®": "LN", "CKD": "CKD"}
        for k, v in disease_map.items():
            if k.lower() in filename.lower(): return v
        return "General"

    def process_pdf(self, file_path: Path) -> List[Document]:
        print(f"  -> æ­£åœ¨è§£æ: {file_path.name}")
        try:
            doc = fitz.open(file_path)
            base_meta = self._extract_metadata(file_path)
            chunks = []
            for i, page in enumerate(doc):
                text = page.get_text().strip()
                if len(text) < 50: continue
                
                # åˆ›å»ºåŸå§‹æ–‡æ¡£å¯¹è±¡
                page_doc = Document(
                    page_content=text, 
                    metadata={**base_meta, "page": i+1}
                )
                # åˆ‡åˆ†
                chunks.extend(self.text_splitter.split_documents([page_doc]))
            doc.close()
            return chunks
        except Exception as e:
            print(f"  âŒ è§£æå¤±è´¥ {file_path.name}: {e}")
            return []

    def run(self, target_category: Optional[str] = None, limit: int = 999):
        """
        target_category: æŒ‡å®šå¤„ç†å“ªä¸ªå­æ–‡ä»¶å¤¹ï¼Œå¦‚ 'Consensus'
        """
        # è·¯å¾„æœç´¢é€»è¾‘ï¼šå¦‚æœæ˜¯ Consensusï¼Œå°±åªæœ raw/Consensus/*.pdf
        search_path = self.base_dir / target_category if target_category else self.base_dir
        print(f"ğŸ” æœç´¢ç›®å½•: {search_path.absolute()}")
        
        # ä½¿ç”¨ rglob è¿›è¡Œé€’å½’æœç´¢æ‰€æœ‰ PDF
        pdf_files = list(search_path.rglob("*.pdf"))[:limit]
        print(f"ğŸ“‚ æ‰¾åˆ° {len(pdf_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        all_chunks = []
        for f in pdf_files:
            all_chunks.extend(self.process_pdf(f))
            
        print(f"âœ¨ å¤„ç†å®Œæˆ: ç”Ÿæˆ {len(all_chunks)} ä¸ª Chunks")
        return all_chunks

if __name__ == "__main__":
    from retrieval import MedicalVectorStore
    
    project_root = Path(__file__).resolve().parent.parent
    raw_data_dir = project_root / "data" / "raw"
    vector_db_dir = str(project_root / "data" / "vector_store") # æ•°æ®åº“è·¯å¾„
    
    ingestor = MedicalIngestor(raw_data_dir)
    
    # 1. æå– Consensus
    print("ğŸš€ å¼€å§‹è§£æ PDF...")
    consensus_chunks = ingestor.run(target_category="Consensus")
    
    # 2. å­˜å…¥æ•°æ®åº“
    if consensus_chunks:
        print(f"ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“å¹¶å­˜å…¥ {len(consensus_chunks)} ä¸ª Chunks...")
        db_manager = MedicalVectorStore(vector_db_dir)
        db_manager.add_documents(consensus_chunks)
        print("âœ… æ•°æ®åº“æ„å»ºå®Œæˆï¼")
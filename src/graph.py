import os
from typing import List, TypedDict
from pathlib import Path

from langgraph.graph import StateGraph, END
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

# å¯¼å…¥ä½ ä¹‹å‰å†™çš„æ¨¡å—
from retrieval import MedicalVectorStore
from chains import MedicalRAGChain

# 1. å®šä¹‰çŠ¶æ€ (State)
class GraphState(TypedDict):
    question: str
    documents: List[Document]
    generation: str
    # å¯ä»¥åœ¨è¿™é‡Œæ‰©å±•ï¼Œæ¯”å¦‚ï¼š
    # steps: List[str] 

# 2. å®šä¹‰è¾…åŠ©ç»„ä»¶ï¼šæ–‡æ¡£è¯„åˆ†å™¨ (Grader)
# è¿™ä¸€æ­¥æ˜¯è§£å†³å¹»è§‰çš„å…³é”®ï¼šåˆ¤æ–­æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦çœŸçš„å’Œé—®é¢˜ç›¸å…³
class GradeAnswer(BaseModel):
    """ç”¨äºåˆ¤æ–­æ–‡æ¡£ç›¸å…³æ€§çš„äºŒå…ƒè¯„åˆ†ç»“æ„"""
    binary_score: str = Field(description="ç›¸å…³æ€§è¯„åˆ†ï¼Œ'yes' æˆ– 'no'")

def get_grader():
    llm = ChatOpenAI(
        model="qwen-flash", 
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        temperature=0
    )
    # å¼ºåˆ¶ LLM è¾“å‡ºç»“æ„åŒ–æ•°æ®
    structured_llm = llm.with_structured_output(GradeAnswer)
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ–‡çŒ®è´¨é‡è¯„ä¼°å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ç»™å®šçš„å‚è€ƒèµ„æ–™æ˜¯å¦ä¸ç”¨æˆ·çš„é—®é¢˜ç›´æ¥ç›¸å…³ã€‚
    å¦‚æœæ˜¯åŒ»å­¦ä¸Šçš„ç›¸å…³ä¿¡æ¯ï¼ˆå¦‚ç—…å› ã€è¯Šæ–­ã€æ²»ç–—å»ºè®®ç­‰ï¼‰ï¼Œè¯·å›ç­” 'yes'ï¼Œå¦åˆ™å›ç­” 'no'ã€‚"""
    
    grader_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "å‚è€ƒèµ„æ–™: \n\n {document} \n\n ç”¨æˆ·é—®é¢˜: {question}")
    ])
    
    return grader_prompt | structured_llm

# 3. å®šä¹‰èŠ‚ç‚¹ (Nodes)

# åˆå§‹åŒ–ç»„ä»¶ (å…¨å±€ï¼Œé¿å…é‡å¤åŠ è½½)
project_root = Path(__file__).resolve().parent.parent
db_dir = str(project_root / "data" / "vector_store")
store_manager = MedicalVectorStore(db_dir)
rag_chain_manager = MedicalRAGChain(store_manager)
doc_grader = get_grader()

def retrieve(state: GraphState):
    print("--- æ­¥éª¤1ï¼šæ‰§è¡Œå‘é‡æœç´¢ ---")
    question = state["question"]
    # æœç´¢ Top-5 ç‰‡æ®µ
    results = store_manager.search(question, k=5)
    # æ³¨æ„ï¼šsearch è¿”å›çš„æ˜¯ (doc, score) å…ƒç»„ï¼Œæˆ‘ä»¬åªéœ€è¦ doc
    docs = [res[0] for res in results]
    return {"documents": docs, "question": question}

def grade_documents(state: GraphState):
    print("--- æ­¥éª¤2ï¼šè¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§ ---")
    question = state["question"]
    documents = state["documents"]
    
    filtered_docs = []
    for d in documents:
        # è°ƒç”¨è¯„åˆ†å™¨
        score = doc_grader.invoke({"question": question, "document": d.page_content})
        if score.binary_score == "yes":
            print(f"  [âœ“] ç›¸å…³: {d.metadata.get('source')[:20]}...")
            filtered_docs.append(d)
        else:
            print(f"  [âœ—] æ— å…³: {d.metadata.get('source')[:20]}...")
            
    return {"documents": filtered_docs, "question": question}

def generate(state: GraphState):
    print("--- æ­¥éª¤3ï¼šç”Ÿæˆæœ€ç»ˆå›å¤ ---")
    question = state["question"]
    docs = state["documents"]
    
    if not docs:
        return {"generation": "æŠ±æ­‰ï¼Œæˆ‘åœ¨æä¾›çš„ä¸´åºŠæŒ‡å—ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸è¯¥é—®é¢˜ç›¸å…³çš„ç¡®åˆ‡ä¿¡æ¯ã€‚", "documents": docs}
    
    # å¤ç”¨ä¹‹å‰ chains.py é‡Œçš„é€»è¾‘
    rag_chain = rag_chain_manager.get_chain()
    # æ³¨æ„ï¼šæˆ‘ä»¬çš„ chain å†…éƒ¨å·²ç»åŒ…å«äº† retrieverã€‚
    # è¿™é‡Œä¸ºäº†æ¼”ç¤º Graphï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨ LLMï¼Œæˆ–è€…ä¿®æ”¹ chain æ¥æ”¶ docs 
    # è¿™é‡Œé‡‡ç”¨ç›´æ¥è°ƒç”¨å°è£…å¥½çš„ chain (å®ƒä¼šäºŒæ¬¡æœç´¢ï¼Œä½†æ›´ç¨³å®š)
    generation = rag_chain.invoke(question)
    
    return {"generation": generation, "documents": docs}

# 4. æ„å»ºå›¾ (Graph Construction)

workflow = StateGraph(GraphState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_docs", grade_documents)
workflow.add_node("generate", generate)

# å»ºç«‹è¿æ¥
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "grade_docs")
workflow.add_edge("grade_docs", "generate")
workflow.add_edge("generate", END)

# ç¼–è¯‘åº”ç”¨
app = workflow.compile()

# 5. è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    test_question = "å¯¹äºé«˜é’¾è¡€ç—‡æ‚£è€…ï¼Œè¡€æ¶²é€ææ˜¯å¦‚ä½•æ¸…é™¤é’¾ç¦»å­çš„ï¼Ÿ"
    
    print(f"\nğŸš€ å¯åŠ¨åŒ»ç–— RAG å·¥ä½œæµ \né—®é¢˜: {test_question}\n")
    
    # è®¾ç½®ä¸€ä¸ªé€’å½’ä¸Šé™ï¼ˆè™½ç„¶åé¢åªæœ‰ç›´çº¿é€»è¾‘ï¼Œä½†åœ¨å¾ªç¯å›¾ä¸­å¾ˆé‡è¦ï¼‰
    config = {"recursion_limit": 10}
    
    for output in app.stream({"question": test_question}, config):
        # æ‰“å°å½“å‰æ­£åœ¨è¿è¡Œçš„èŠ‚ç‚¹å
        for key, value in output.items():
            pass # èŠ‚ç‚¹å†…éƒ¨å·²ç»æœ‰æ‰“å°è¾“å‡ºäº†
            
    final_result = app.invoke({"question": test_question})
    print("\n" + "â˜…" * 30 + " æœ€ç»ˆè¯Šæ–­å»ºè®® " + "â˜…" * 30)
    print(final_result["generation"])